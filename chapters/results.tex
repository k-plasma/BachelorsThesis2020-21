\section{Neural Networks}

In this section, I will describe the Progression from Normal Neural network to Recurring Neural Networks - RNN. In the end, I'll show that the GRU works well and I have solved what I set out to do. 
it should be here


In this section, various results of this research activity will be presented. The results will majorly be training and validation performance. From the above discussion, it should be noticed that GRU is performing extremely better than both feed-forward network and LSTM network. So, in the following discussion, performance of GRU will be described only.  

\subsection{Training Results}

Training results pertain to how quickly and efficiently a given networks understands and learns its environment. The training efficiency of GRU is depicted in Figure ~\ref{fig:trg}, epoch number vs learning loss. The learning loss drastically decreases right from first epoch. Rapid decline continues until 40th epoch, then slowly reaches to zero before 100th epoch. This clearly indicates the superiority of GRU over LSTM and FFNN (feed-forward neural network). This performance clearly hints the best prediction accuracy of GRU.  



\subsection{Validation}

Validation results are also shown in Figure ~\ref{fig:trg}, with x-axis showing the epoch number and y-axis showing the prediction error. 150 epochs have been employed to validate the model. Although validation does not approach to zeros exactly, but such a small error could be acceptable for the estimation of GPV. So, it does validate that GRU can perform the best amongst the class of recurrent neural networks. It looks like the efficient use of long-short memory has outclassed the performance of other networks. 


\subsection{Subsection title}
Neural Networks and more advanced technologies. Right now we use Pandas for everything and it's enough, except when it's not. The more advanced we go into tech, the more basic the Machine Learning tools.

Some Machine Learning models are very dependent on used materials. SCI-KIT uses the most basic of Neural networks. What you'll notice from GPV-Take 1 is that you can't create modern systems with SCI-KIT because it is the highest level of abstraction.

Here, I'll need to show the results of the Google Colab Notebook - Gross Production Value 1 and the abysmal results I got using SCI-KIT

We use SCI-KIT for scaling. It implements it very well. Train - Test - Split
\cite{ponce1989engineering}


\subsection{Subsection title}
